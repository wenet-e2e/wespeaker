### train configuraton

exp_dir: exp/ECAPA_TDNN_GLOB_c512-ASTP-emb192-fbank80-num_frms200-aug1.0-spTrue-saFalse-SimCLR-SGD-epoch150
gpus: "[0,1]"
num_avg: 10
enable_amp: False # whether enable automatic mixed precision training

seed: 42
num_epochs: 150
save_epoch_interval: 5 # save model every 5 epochs
log_batch_interval: 100 # log every 100 batchs

dataloader_args:
  batch_size: 128
  num_workers: 16
  pin_memory: False
  prefetch_factor: 4
  drop_last: True

dataset_args:
  # the sample number which will be traversed within one epoch, if the value equals to 0,
  # the utterance number in the dataset will be used as the sample_num_per_epoch.
  sample_num_per_epoch: 0
  shuffle: True
  shuffle_args:
    shuffle_size: 2500
  resample_rate: 16000
  speed_perturb: True
  chunk_info_args:
    global_chunk_len: 200
    global_chunk_num: 1
    local_chunk_len: 200
    local_chunk_num: 1
  aug_prob: 1.0 # prob to add reverb & noise aug per sample
  fbank_args:
    num_mel_bins: 80
    frame_shift: 10
    frame_length: 25
    dither: 1.0
  spec_aug: False
  spec_aug_args:
    num_t_mask: 1
    num_f_mask: 1
    max_t: 10
    max_f: 8
    prob: 0.6

model: ECAPA_TDNN_GLOB_c512 # ECAPA_TDNN_GLOB_c512, ECAPA_TDNN_GLOB_c1024
model_init: null
model_args:
  feat_dim: 80
  embed_dim: 192
  pooling_func: "ASTP"
contrastive_type: "simclr" # simclr, moco
simclr_args:
  T: 0.07
  mlp: False
moco_args:
  K: 65536
  m: 0.999
  T: 0.07
  mlp: False

optimizer: SGD
optimizer_args:
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001

scheduler: ExponentialDecrease
scheduler_args:
  initial_lr: 0.1
  final_lr: 0.00005
  warm_up_epoch: 6
  warm_from_zero: True
